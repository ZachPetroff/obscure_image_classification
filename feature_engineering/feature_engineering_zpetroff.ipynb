{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feature_engineering_zpetroff",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "from scipy import signal\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import skimage.measure\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "1JLGcCzBfztc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blurry_ims = os.listdir('blurred')\n",
        "nonblurry_ims = os.listdir('clear')"
      ],
      "metadata": {
        "id": "6w5flT-GSKQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get variance\n",
        "def get_imvar(image):\n",
        "  image = np.array(image)\n",
        "  return np.var(image)\n",
        "\n",
        "def get_imvar2(image):\n",
        "  image = image.convert(\"L\")\n",
        "  image = np.array(image)\n",
        "  return sum((image.flatten() - np.linalg.norm(image.flatten()))**2)\n",
        "\n",
        "def get_localvar(image, box_size=40):\n",
        "  image = np.array(image)\n",
        "  lvar = []\n",
        "  for i in range(round(480/box_size)):\n",
        "    for j in range(round(640/box_size)):\n",
        "      sqr = image[i:i+40][j:j+40]\n",
        "      flat = sqr.flatten()\n",
        "      lvar.append(np.var(flat))\n",
        "  return sum(lvar)\n",
        "\n",
        "def get_localvar2(image, box_size=40):\n",
        "  image = image.convert(\"L\")\n",
        "  image = np.array(image)\n",
        "  lvar = []\n",
        "  for i in range(round(480/box_size)):\n",
        "    for j in range(round(640/box_size)):\n",
        "      sqr = image[i:i+40][j:j+40]\n",
        "      flat = sqr.flatten()\n",
        "      lvar.append(sum((flat - np.linalg.norm(flat)) ** 2))\n",
        "  return (sum(lvar))\n",
        "\n",
        "def element_oper(img1, img2, oper):\n",
        "    ''' performs given element-by-element operation on \n",
        "        the two given images/matrices. '''\n",
        "    if len(img1) != len(img2) or len(img1[0]) != len(img2[0]):\n",
        "        print(\"Images must be the same size.\")\n",
        "    ret_mat = np.zeros((len(img1),len(img1[0])))\n",
        "    for i in range(len(img1)):\n",
        "        for j in range(len(img1[0])):\n",
        "            p1 = img1[i, j]\n",
        "            p2 = img2[i, j]\n",
        "            ret_mat[i, j] = oper(p1,p2)\n",
        "    return ret_mat\n",
        "\n",
        "def find_min(v1, v2):\n",
        "    ''' returns min of v1 and v2. Used to find min eigen-values. '''\n",
        "    return min(v1, v2)\n",
        "\n",
        "def harris_detection(I, T):\n",
        "    ''' Implements harris corner detection algo. '''\n",
        "    # used to find partial derivatives\n",
        "    sobel_x = [[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]\n",
        "    sobel_y = [[1, 1, 1], [0, 0, 0], [-1, -1, -1]]\n",
        "    \n",
        "    # used to calculate A, B, and C\n",
        "    box = [[1,1,1,1,1],\n",
        "           [1,1,1,1,1],\n",
        "           [1,1,1,1,1],\n",
        "           [1,1,1,1,1],\n",
        "           [1,1,1,1,1]]\n",
        "    \n",
        "    # convert to grayscale\n",
        "    I = I.convert(\"L\")\n",
        "    \n",
        "    # convert to NP array, so we can have values > 255\n",
        "    im_array = np.array(I)\n",
        "\n",
        "    # find partial derivatives\n",
        "    Ix = signal.fftconvolve(im_array, sobel_x)\n",
        "    Iy = signal.fftconvolve(im_array, sobel_y)\n",
        "\n",
        "    # find A, B, and C\n",
        "    A = Ix * Ix\n",
        "    A = signal.fftconvolve(A, box)\n",
        "    B = Ix * Iy\n",
        "    B = signal.fftconvolve(B, box)\n",
        "    C = Iy * Iy\n",
        "    C = signal.fftconvolve(C, box)\n",
        "    # Find eigenvalues\n",
        "    trace = A + C\n",
        "    det = A*C-B*B\n",
        "\n",
        "    half_trace = trace / 2\n",
        "    S = (trace*trace / 4 - det) ** .5\n",
        "    \n",
        "    lambda1 = half_trace + S\n",
        "    lambda2 = half_trace - S \n",
        "    \n",
        "    # find minimum eigenvalues\n",
        "    min_eigens = element_oper(lambda1, lambda2, find_min)\n",
        "    \n",
        "    corner_mat = np.zeros((len(min_eigens),len(min_eigens[0])))\n",
        "    # Apply threshold\n",
        "    for i in range(len(corner_mat)):\n",
        "        for j in range(len(corner_mat[0])):\n",
        "            p = min_eigens[i,j]\n",
        "            if p > T:\n",
        "                corner_mat[i,j] = min_eigens[i,j] \n",
        "\n",
        "    return corner_mat\n",
        "\n",
        "def count_nonzero(corners):\n",
        "  total = 0\n",
        "  corners = corners.flatten()\n",
        "  for i in range(len(corners)):\n",
        "    if corners[i] != 0:\n",
        "      total+=1\n",
        "  return total\n",
        "\n",
        "def get_corneravg(corners):\n",
        "  corners = corners.flatten()\n",
        "  return sum(corners) / len(corners)\n",
        "\n",
        "def get_entropy(image):\n",
        "  image = np.array(image)\n",
        "  return skimage.measure.shannon_entropy(image.flatten())\n",
        "\n",
        "def get_perimeter(image):\n",
        "  image = image.convert(\"L\")\n",
        "  image = np.array(image)\n",
        "  return skimage.measure.perimeter_crofton(image)\n",
        "\n",
        "def get_std(image):\n",
        "  image = np.array(image)\n",
        "  return np.std(image.flatten())\n",
        "\n",
        "def get_partial_sum(image):\n",
        "  image = image.convert(\"L\")\n",
        "  image = np.array(image)\n",
        "  px = signal.fftconvolve(image, [[-1,-1,-1], [0,0,0], [1,1,1]])\n",
        "  py = signal.fftconvolve(image, [[-1,0,1], [-1,0,1], [-1,0,1]])\n",
        "  p = px+py\n",
        "  return np.var(p.flatten())\n",
        "\n",
        "imgs = []\n",
        "vars = []\n",
        "vars2 = []\n",
        "lvars = []\n",
        "lvars2 = []\n",
        "entropies = []\n",
        "perims = []\n",
        "stds = []\n",
        "p_sums = []\n",
        "num_corners = []\n",
        "avg_corners = []\n",
        "classes = []\n",
        "\n",
        "for im in blurry_ims:\n",
        "  curr = Image.open(\"blurred/\"+im)\n",
        "  imgs.append(im)\n",
        "  vars.append(get_imvar(curr))\n",
        "  vars2.append(get_imvar2(curr))\n",
        "  lvars.append(get_localvar(curr))\n",
        "  lvars2.append(get_localvar2(curr))\n",
        "  entropies.append(get_entropy(curr))\n",
        "  perims.append(get_perimeter(curr))\n",
        "  stds.append(get_std(curr))\n",
        "  p_sums.append(get_partial_sum(curr))\n",
        "  corners = harris_detection(curr, 5000)\n",
        "  num_corners.append(count_nonzero(corners))\n",
        "  avg_corners.append(get_corneravg(corners))\n",
        "  classes.append(1)\n",
        "\n",
        "for im in nonblurry_ims:\n",
        "  curr = Image.open(\"clear/\"+im)\n",
        "  imgs.append(im)\n",
        "  vars.append(get_imvar(curr))\n",
        "  vars2.append(get_imvar2(curr))\n",
        "  lvars.append(get_localvar(curr))\n",
        "  lvars2.append(get_localvar2(curr))\n",
        "  entropies.append(get_entropy(curr))\n",
        "  perims.append(get_perimeter(curr))\n",
        "  stds.append(get_std(curr))\n",
        "  p_sums.append(get_partial_sum(curr))\n",
        "  corners = harris_detection(curr, 5000)\n",
        "  num_corners.append(count_nonzero(corners))\n",
        "  avg_corners.append(get_corneravg(corners))\n",
        "  classes.append(0)\n",
        "\n",
        "feature_dict = {\"Variance\": np.array(vars)/max(vars), \"EM Variance\": np.array(vars2)/max(vars2), \"Local Variance\": np.array(lvars)/max(lvars), \"EM Local Variance\": np.array(lvars2)/max(lvars2), \n",
        "                \"Entropy\": np.array(entropies)/max(entropies), \"Crofton Perimeter\": np.array(perims)/max(perims), \"Standard Deviation\": np.array(stds)/max(stds), \"Partial Derivative Sum\": np.array(p_sums)/max(p_sums),\n",
        "                \"Number of Corners\": np.array(num_corners)/max(num_corners), \"Average Corner Strength\": np.array(avg_corners)/max(avg_corners), \"Class\": classes}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sy-WMX1RSLj9",
        "outputId": "3920da0b-95dc-4d27-93dc-3fecd679323c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in sqrt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_dict = {\"Variance\": vars/max(vars), \"EM Variance\": vars2/max(vars2), \"Local Variance\": lvars/max(lvars), \"EM Local Variance\": lvars2/max(lvars2), \n",
        "                \"Entropy\": entropies/max(entropies), \"Crofton Perimeter\": perims/max(perims), \"Standard Deviation\": stds/max(stds), \"Partial Derivative Sum\": p_sums/max(p_sums),\n",
        "                \"Number of Corners\": np.array(num_corners)/max(num_corners), \"Average Corner Strength\": avg_corners/max(avg_corners), \"Class\": classes}\n",
        "for i in feature_dict:\n",
        "  print(max(feature_dict[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOZwzRBeXzlm",
        "outputId": "2979cd49-447a-4408-b91b-2865aedadf4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_df = pd.DataFrame(feature_dict, index=imgs)\n",
        "feature_df.to_csv(\"Img_Features.csv\")"
      ],
      "metadata": {
        "id": "tVQLXrtszUxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = pd.read_csv(\"Img_Features_Combined.csv\")\n",
        "for feat in feature_dict:\n",
        "  new_df[feat] = feature_dict[feat]\n",
        "\n",
        "new_feats = ['Sobel V Color Avg', 'Sobel V Gray Avg', 'Sobel H Color Avg', 'Sobel H Gray Avg', 'Laplacian Positive Color Avg',\n",
        "      'Laplacian Positive Gray Avg', 'Laplacian Negative Color Avg', 'Laplacian Negative Gray Avg']\n",
        "\n",
        "for feat in new_feats:\n",
        "  new_df[feat] = (new_df[feat]-new_df[feat].min())/(new_df[feat].max()-new_df[feat].min())\n",
        "\n",
        "new_df.index = imgs\n",
        "new_df = new_df.drop([\"Unnamed: 0\"], axis=1)\n",
        "new_df.head()\n",
        "new_df.to_csv(\"Img_Features_Norm.csv\")"
      ],
      "metadata": {
        "id": "jVaL-mmoRd7-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "e76fce0b-d6a5-4a0b-9c1e-aaeb821a27c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-c5eb3146a622>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Img_Features_Combined.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mnew_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m new_feats = ['Sobel V Color Avg', 'Sobel V Gray Avg', 'Sobel H Color Avg', 'Sobel H Gray Avg', 'Laplacian Positive Color Avg',\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Img_Features_Combined.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WytWP0z54CAx"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "from scipy import signal\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import skimage.measure"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1D Plots for:\n",
        "\n",
        "- Global Variance\n",
        "- Local Variance Difference\n",
        "- Number of Edges\n",
        "- Average Brightness of Edge\n",
        "- Entropy\n",
        "- Crofton Perimeter\n",
        "- Standard Deviation\n",
        "\n",
        "Examples For:\n",
        "\n",
        "- Partial Derivatives\n",
        "- Harris Edge Detection"
      ],
      "metadata": {
        "id": "zPltGqgVQfVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Image Files"
      ],
      "metadata": {
        "id": "r9dJkj2cWHN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blurry_ims = os.listdir('Blurry')\n",
        "nonblurry_ims = os.listdir('Non-Blurry')\n",
        "\n",
        "np.random.seed(0) #3\n",
        "sample_size = 50\n",
        "blurry_sample = np.random.choice(blurry_ims, size=sample_size, replace=False)\n",
        "nonblurry_sample = np.random.choice(nonblurry_ims, size=50, replace=False)\n",
        "print(blurry_sample)\n",
        "print(nonblurry_sample)"
      ],
      "metadata": {
        "id": "wguIWn_qQcPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot Global Variance"
      ],
      "metadata": {
        "id": "H2Aln-vwWKlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b_vars = np.zeros(sample_size)\n",
        "nb_vars = np.zeros(sample_size)\n",
        "\n",
        "for im in range(sample_size):\n",
        "  blurry = Image.open(\"Blurry/\"+blurry_sample[im])\n",
        "  blurry = np.array(blurry)\n",
        "  \n",
        "  nonblurry = Image.open(\"Non-Blurry/\"+nonblurry_sample[im])\n",
        "  nonblurry = np.array(nonblurry)\n",
        "  \n",
        "  b_vars[im] = np.var(blurry.flatten())\n",
        "  nb_vars[im] = np.var(nonblurry.flatten())\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(b_vars, [1]*len(b_vars), \"ro\", label=\"Blurry\")\n",
        "ax.plot(nb_vars, [1]*len(nb_vars), \"bo\", label=\"Non-Blurry\")\n",
        "plt.style.use('seaborn')\n",
        "plt.title(\"Global Variance\", fontsize=20)\n",
        "fig.set_size_inches(15,2)\n",
        "ax.grid(True)\n",
        "ax.legend()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "44bQNyMBV0uX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b_vars2 = np.zeros(sample_size)\n",
        "nb_vars2 = np.zeros(sample_size)\n",
        "\n",
        "for im in range(sample_size):\n",
        "  blurry = Image.open(\"Blurry/\"+blurry_sample[im])\n",
        "  blurry = blurry.convert(\"L\")\n",
        "  blurry = np.array(blurry)\n",
        "  \n",
        "  nonblurry = Image.open(\"Non-Blurry/\"+nonblurry_sample[im])\n",
        "  nonblurry = nonblurry.convert(\"L\")\n",
        "  nonblurry = np.array(nonblurry)\n",
        "  \n",
        "  b_vars2[im] = sum((blurry.flatten() - np.linalg.norm(blurry.flatten()))**2) \n",
        "  nb_vars2[im] = sum((nonblurry.flatten() - np.linalg.norm(nonblurry.flatten()))**2) \n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(b_vars2, [1]*len(b_vars), \"ro\", label=\"Blurry\")\n",
        "ax.plot(nb_vars2, [1]*len(nb_vars), \"bo\", label=\"Non-Blurry\")\n",
        "plt.style.use('seaborn')\n",
        "plt.title(\"Global Variance (As Defined in Efficient Method Paper)\", fontsize=20)\n",
        "fig.set_size_inches(15,2)\n",
        "ax.grid(True)\n",
        "ax.legend()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "6CvHArpqveKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "box_size = 40\n",
        "b_lvars = []\n",
        "nb_lvars = []\n",
        "\n",
        "for im in range(sample_size):\n",
        "  b_lvar = []\n",
        "  nb_lvar = []\n",
        "  blurry = Image.open(\"Blurry/\"+blurry_sample[im])\n",
        "  blurry = np.array(blurry)\n",
        "  nonblurry = Image.open(\"Non-Blurry/\"+nonblurry_sample[im])\n",
        "  nonblurry = np.array(nonblurry)\n",
        "  for i in range(round(480/box_size)):\n",
        "    for j in range(round(640/box_size)):\n",
        "      blur_sqr = blurry[i:i+40][j:j+40]\n",
        "      blur_flat = blur_sqr.flatten()\n",
        "      b_lvar.append(np.var(blur_flat))\n",
        "\n",
        "      nonblur_sqr = nonblurry[i:i+40][j:j+40]\n",
        "      nonblur_flat = nonblur_sqr.flatten()\n",
        "      nb_lvar.append(np.var(nonblur_flat))\n",
        "  b_lvars.append(sum(b_lvar))\n",
        "  nb_lvars.append(sum(nb_lvar))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(b_lvars, [1]*len(b_lvars), \"ro\", label=\"Blurry\")\n",
        "ax.plot(nb_lvars, [1]*len(nb_lvars), \"bo\", label=\"Non-Blurry\")\n",
        "plt.style.use('seaborn')\n",
        "plt.title(\"Sum of Local Variances\", fontsize=20)\n",
        "fig.set_size_inches(15,2)\n",
        "ax.grid(True)\n",
        "ax.legend()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "vzLo7rnIdgfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "box_size = 40\n",
        "b_lvars2 = []\n",
        "nb_lvars2 = []\n",
        "\n",
        "for im in range(sample_size):\n",
        "  b_lvar = []\n",
        "  nb_lvar = []\n",
        "  blurry = Image.open(\"Blurry/\"+blurry_sample[im])\n",
        "  blurry = blurry.convert(\"L\")\n",
        "  blurry = np.array(blurry)\n",
        "  nonblurry = Image.open(\"Non-Blurry/\"+nonblurry_sample[im])\n",
        "  nonblurry = nonblurry.convert(\"L\")\n",
        "  nonblurry = np.array(nonblurry)\n",
        "  for i in range(round(480/box_size)):\n",
        "    for j in range(round(640/box_size)):\n",
        "      blur_sqr = blurry[i:i+40][j:j+40]\n",
        "      blur_flat = blur_sqr.flatten()\n",
        "      b_lvar.append(sum((blur_flat - np.linalg.norm(blur_flat)) ** 2))\n",
        "\n",
        "      nonblur_sqr = nonblurry[i:i+40][j:j+40]\n",
        "      nonblur_flat = nonblur_sqr.flatten()\n",
        "      nb_lvar.append(sum((nonblur_flat - np.linalg.norm(nonblur_flat))** 2))\n",
        "  b_lvars2.append(sum(b_lvar))\n",
        "  nb_lvars2.append(sum(nb_lvar))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(b_lvars2, [1]*len(b_lvars), \"ro\", label=\"Blurry\")\n",
        "ax.plot(nb_lvars2, [1]*len(nb_lvars), \"bo\", label=\"Non-Blurry\")\n",
        "plt.style.use('seaborn')\n",
        "plt.title(\"Sum of Local Variances (As Defined in Efficient Method Paper)\", fontsize=20)\n",
        "fig.set_size_inches(15,2)\n",
        "ax.grid(True)\n",
        "ax.legend()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "TC78kbUPxs0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def element_oper(img1, img2, oper):\n",
        "    ''' performs given element-by-element operation on \n",
        "        the two given images/matrices. '''\n",
        "    if len(img1) != len(img2) or len(img1[0]) != len(img2[0]):\n",
        "        print(\"Images must be the same size.\")\n",
        "    ret_mat = np.zeros((len(img1),len(img1[0])))\n",
        "    for i in range(len(img1)):\n",
        "        for j in range(len(img1[0])):\n",
        "            p1 = img1[i, j]\n",
        "            p2 = img2[i, j]\n",
        "            ret_mat[i, j] = oper(p1,p2)\n",
        "    return ret_mat\n",
        "\n",
        "def find_min(v1, v2):\n",
        "    ''' returns min of v1 and v2. Used to find min eigen-values. '''\n",
        "    return min(v1, v2)\n",
        "\n",
        "def harris_detection(I, T):\n",
        "    ''' Implements harris corner detection algo. '''\n",
        "    # used to find partial derivatives\n",
        "    sobel_x = [[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]\n",
        "    sobel_y = [[1, 1, 1], [0, 0, 0], [-1, -1, -1]]\n",
        "    \n",
        "    # used to calculate A, B, and C\n",
        "    box = [[1,1,1,1,1],\n",
        "           [1,1,1,1,1],\n",
        "           [1,1,1,1,1],\n",
        "           [1,1,1,1,1],\n",
        "           [1,1,1,1,1]]\n",
        "    \n",
        "    # convert to grayscale\n",
        "    I = I.convert(\"L\")\n",
        "    \n",
        "    # convert to NP array, so we can have values > 255\n",
        "    im_array = np.array(I)\n",
        "\n",
        "    # find partial derivatives\n",
        "    Ix = signal.fftconvolve(im_array, sobel_x)\n",
        "    Iy = signal.fftconvolve(im_array, sobel_y)\n",
        "\n",
        "    # find A, B, and C\n",
        "    A = Ix * Ix\n",
        "    A = signal.fftconvolve(A, box)\n",
        "    B = Ix * Iy\n",
        "    B = signal.fftconvolve(B, box)\n",
        "    C = Iy * Iy\n",
        "    C = signal.fftconvolve(C, box)\n",
        "    # Find eigenvalues\n",
        "    trace = A + C\n",
        "    det = A*C-B*B\n",
        "\n",
        "    half_trace = trace / 2\n",
        "    S = (trace*trace / 4 - det) ** .5\n",
        "    \n",
        "    lambda1 = half_trace + S\n",
        "    lambda2 = half_trace - S \n",
        "    \n",
        "    # find minimum eigenvalues\n",
        "    min_eigens = element_oper(lambda1, lambda2, find_min)\n",
        "    \n",
        "    corner_mat = np.zeros((len(min_eigens),len(min_eigens[0])))\n",
        "    # Apply threshold\n",
        "    for i in range(len(corner_mat)):\n",
        "        for j in range(len(corner_mat[0])):\n",
        "            p = min_eigens[i,j]\n",
        "            if p > T:\n",
        "                corner_mat[i,j] = min_eigens[i,j] \n",
        "\n",
        "    return corner_mat\n",
        "\n",
        "def count_nonzero(corners):\n",
        "  total = 0\n",
        "  for i in range(len(corners)):\n",
        "    for j in range(len(corners[0])):\n",
        "      if corners[i][j] != 0:\n",
        "        total+=1\n",
        "  return total\n",
        "\n",
        "b_corners = np.zeros(sample_size)\n",
        "nb_corners = np.zeros(sample_size)\n",
        "\n",
        "for im in range(sample_size):\n",
        "  blur_corners = harris_detection(Image.open(\"Blurry/\"+blurry_sample[im]), 5000)\n",
        "  blur_corners = count_nonzero(blur_corners)\n",
        "  nblur_corners = harris_detection(Image.open(\"Non-Blurry/\"+nonblurry_sample[im]), 5000)\n",
        "  nblur_corners = count_nonzero(nblur_corners)\n",
        "  b_corners[im] = blur_corners\n",
        "  nb_corners[im] = nblur_corners\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(b_corners, [1]*len(b_corners), \"ro\", label=\"Blurry\")\n",
        "ax.plot(nb_corners, [1]*len(nb_corners), \"bo\", label=\"Non-Blurry\")\n",
        "plt.style.use('seaborn')\n",
        "plt.title(\"Numbers of Corners\", fontsize=20)\n",
        "fig.set_size_inches(15,2)\n",
        "ax.grid(True)\n",
        "ax.legend()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "5eGx7m2FgzMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b_corneravg = np.zeros(sample_size)\n",
        "nb_corneravg = np.zeros(sample_size)\n",
        "\n",
        "for im in range(sample_size):\n",
        "  blur = Image.open(\"Blurry/\"+blurry_sample[im])\n",
        "  blur_corner = harris_detection(blur, 5000)\n",
        "  b_corneravg[im] = sum(blur_corner.flatten()) / b_corners[im]\n",
        "  \n",
        "  nblur = Image.open(\"Non-Blurry/\"+nonblurry_sample[im])\n",
        "  nblur_corner = harris_detection(nblur, 5000)\n",
        "  nb_corneravg[im] = sum(nblur_corner.flatten()) / nb_corners[im]\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(b_corneravg, [1]*len(b_corneravg), \"ro\", label=\"Blurry\")\n",
        "ax.plot(nb_corneravg, [1]*len(nb_corneravg), \"bo\", label=\"Non-Blurry\")\n",
        "plt.style.use('seaborn')\n",
        "plt.title(\"Average Corner Strength\", fontsize=20)\n",
        "fig.set_size_inches(15,2)\n",
        "ax.grid(True)\n",
        "ax.legend()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "IY8pHxk7xAV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b_entropies = np.zeros(sample_size)\n",
        "nb_entropies = np.zeros(sample_size)\n",
        "\n",
        "for im in range(sample_size):\n",
        "  blur = Image.open(\"Blurry/\"+blurry_sample[im])\n",
        "  blur = np.array(blur)\n",
        "  b_entropies[im] = skimage.measure.shannon_entropy(blur.flatten())\n",
        "  \n",
        "  nblur = Image.open(\"Non-Blurry/\"+nonblurry_sample[im])\n",
        "  nblur = np.array(nblur)\n",
        "  nb_entropies[im] = skimage.measure.shannon_entropy(nblur.flatten())\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(b_entropies, [1]*len(b_entropies), \"ro\", label=\"Blurry\")\n",
        "ax.plot(nb_entropies, [1]*len(nb_entropies), \"bo\", label=\"Non-Blurry\")\n",
        "plt.style.use('seaborn')\n",
        "plt.title(\"Entropy\", fontsize=20)\n",
        "fig.set_size_inches(15,2)\n",
        "ax.grid(True)\n",
        "ax.legend()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "POtTYG-F2A5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b_perims = np.zeros(sample_size)\n",
        "nb_perims = np.zeros(sample_size)\n",
        "\n",
        "for im in range(sample_size):\n",
        "  blur = Image.open(\"Blurry/\"+blurry_sample[im])\n",
        "  blur = blur.convert(\"L\")\n",
        "  blur = np.array(blur)\n",
        "  perim = skimage.measure.perimeter_crofton(blur)\n",
        "  b_perims[im] = perim\n",
        "\n",
        "  nblur = Image.open(\"Non-Blurry/\"+nonblurry_sample[im])\n",
        "  nblur = nblur.convert(\"L\")\n",
        "  nblur = np.array(nblur)\n",
        "  perim = skimage.measure.perimeter_crofton(nblur)\n",
        "  nb_perims[im] = perim\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(b_perims, [1]*len(b_entropies), \"ro\", label=\"Blurry\")\n",
        "ax.plot(nb_perims, [1]*len(nb_entropies), \"bo\", label=\"Non-Blurry\")\n",
        "plt.style.use('seaborn')\n",
        "plt.title(\"Crofton Perimeter\", fontsize=20)\n",
        "fig.set_size_inches(15,2)\n",
        "ax.grid(True)\n",
        "ax.legend()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "xJThd6g-3jAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b_stds = np.zeros(sample_size)\n",
        "nb_stds = np.zeros(sample_size)\n",
        "\n",
        "for im in range(sample_size):\n",
        "  blur = Image.open(\"Blurry/\"+blurry_sample[im])\n",
        "  blur = np.array(blur)\n",
        "  std = np.std(blur.flatten())\n",
        "  b_stds[im] = std\n",
        "\n",
        "  nblur = Image.open(\"Non-Blurry/\"+nonblurry_sample[im])\n",
        "  nblur = np.array(nblur)\n",
        "  std = np.std(nblur.flatten())\n",
        "  nb_stds[im] = std\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(b_stds, [1]*len(b_entropies), \"ro\", label=\"Blurry\")\n",
        "ax.plot(nb_stds, [1]*len(nb_entropies), \"bo\", label=\"Non-Blurry\")\n",
        "plt.style.use('seaborn')\n",
        "plt.title(\"Standard Deviation\", fontsize=20)\n",
        "fig.set_size_inches(15,2)\n",
        "ax.grid(True)\n",
        "ax.legend()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "im1XoXfo8rNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blur = Image.open(\"Blurry/\"+blurry_sample[0])\n",
        "blur = blur.convert(\"L\")\n",
        "blur = np.array(blur)\n",
        "py = signal.fftconvolve(blur, [[-1, -1, -1],[0, 0, 0],[1, 1, 1]])\n",
        "px = signal.fftconvolve(blur, [[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]])\n",
        "p = py + px\n",
        "p = Image.fromarray(p)\n",
        "p = p.convert(\"RGB\")\n",
        "p"
      ],
      "metadata": {
        "id": "Hx9brmmsA_GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blur = Image.open(\"Blurry/\"+blurry_sample[1])\n",
        "blur = blur.convert(\"L\")\n",
        "blur = np.array(blur)\n",
        "py = signal.fftconvolve(blur, [[-1, -1, -1],[0, 0, 0],[1, 1, 1]])\n",
        "px = signal.fftconvolve(blur, [[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]])\n",
        "p = py + px\n",
        "p = Image.fromarray(p)\n",
        "p = p.convert(\"RGB\")\n",
        "p"
      ],
      "metadata": {
        "id": "3VZ4XPsTXWXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blur = Image.open(\"Blurry/\"+blurry_sample[3])\n",
        "blur = blur.convert(\"L\")\n",
        "blur = np.array(blur)\n",
        "py = signal.fftconvolve(blur, [[-1, -1, -1],[0, 0, 0],[1, 1, 1]])\n",
        "px = signal.fftconvolve(blur, [[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]])\n",
        "p = py + px\n",
        "p = Image.fromarray(p)\n",
        "p = p.convert(\"RGB\")\n",
        "p"
      ],
      "metadata": {
        "id": "NytIlzaeXaFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nonblur = Image.open(\"Non-Blurry/\"+nonblurry_sample[1])\n",
        "nonblur = nonblur.convert(\"L\")\n",
        "nonblur = np.array(nonblur)\n",
        "py = signal.fftconvolve(nonblur, [[-1, -1, -1],[0, 0, 0],[1, 1, 1]])\n",
        "px = signal.fftconvolve(nonblur, [[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]])\n",
        "p = py + px\n",
        "p = Image.fromarray(p)\n",
        "p = p.convert(\"RGB\")\n",
        "p"
      ],
      "metadata": {
        "id": "nL2hMyFUXciR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nonblur = Image.open(\"Non-Blurry/\"+nonblurry_sample[0])\n",
        "nonblur = nonblur.convert(\"L\")\n",
        "nonblur = np.array(nonblur)\n",
        "py = signal.fftconvolve(nonblur, [[-1, -1, -1],[0, 0, 0],[1, 1, 1]])\n",
        "px = signal.fftconvolve(nonblur, [[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]])\n",
        "p = py + px\n",
        "p = Image.fromarray(p)\n",
        "p = p.convert(\"RGB\")\n",
        "p"
      ],
      "metadata": {
        "id": "hhyOZdSxXv3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nonblur = Image.open(\"Non-Blurry/\"+nonblurry_sample[2])\n",
        "nonblur = nonblur.convert(\"L\")\n",
        "nonblur = np.array(nonblur)\n",
        "py = signal.fftconvolve(nonblur, [[-1, -1, -1],[0, 0, 0],[1, 1, 1]])\n",
        "px = signal.fftconvolve(nonblur, [[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]])\n",
        "p = py + px\n",
        "p = Image.fromarray(p)\n",
        "p = p.convert(\"RGB\")\n",
        "p"
      ],
      "metadata": {
        "id": "2WW3pGzWXyqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b_varps = np.zeros(sample_size)\n",
        "nb_varps = np.zeros(sample_size)\n",
        "\n",
        "for im in range(sample_size):\n",
        "  blur = Image.open(\"Blurry/\"+blurry_sample[im])\n",
        "  blur = blur.convert(\"L\")\n",
        "  blur = np.array(blur)\n",
        "  px = signal.fftconvolve(blur, [[-1, -1, -1],[0, 0, 0],[1, 1, 1]])\n",
        "  py = signal.fftconvolve(blur, [[-1, 0, 1],[-1, 0, 1],[-1, 0, 1]])\n",
        "  p = px + py\n",
        "  avg_p = np.var(p.flatten())\n",
        "  b_varps[im] = avg_p\n",
        "\n",
        "  nblur = Image.open(\"Non-Blurry/\"+nonblurry_sample[im])\n",
        "  nblur = nblur.convert(\"L\")\n",
        "  nblur = np.array(nblur)\n",
        "  px = signal.fftconvolve(nblur, [[-1, -1, -1],[0, 0, 0],[1, 1, 1]])\n",
        "  py = signal.fftconvolve(nblur, [[-1, 0, 1],[-1, 0, 1],[-1, 0, 1]])\n",
        "  p = px + py\n",
        "  avg_p = np.var(p.flatten())\n",
        "  nb_varps[im] = avg_p\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(b_varps, [1]*len(b_entropies), \"ro\", label=\"Blurry\")\n",
        "ax.plot(nb_varps, [1]*len(nb_entropies), \"bo\", label=\"Non-Blurry\")\n",
        "plt.style.use('seaborn')\n",
        "plt.title(\"Partial Derivative Variances\", fontsize=20)\n",
        "fig.set_size_inches(15,2)\n",
        "ax.grid(True)\n",
        "ax.legend()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "PrTJu137X7Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blurry_stats = [b_vars, b_vars2, b_lvars, b_lvars2, b_corners, b_corneravg, b_entropies, b_perims, b_stds, b_varps]\n",
        "nonblurry_stats = [nb_vars, nb_vars2, nb_lvars, nb_lvars2, nb_corners, nb_corneravg, nb_entropies, nb_perims, nb_stds, nb_varps]\n",
        "\n",
        "def data_from_lists(blurry, nonblurry):\n",
        "  X = []\n",
        "  y = []\n",
        "  for im in range(len(blurry[0])): # 25 \n",
        "    b_entry = []\n",
        "    nb_entry = []\n",
        "    for stat in range(len(blurry)):\n",
        "      b_entry.append(blurry[stat][im] / max( max(blurry[stat]), max(nonblurry[stat])  ))\n",
        "      nb_entry.append(nonblurry[stat][im] / max( max(blurry[stat]), max(nonblurry[stat]) ))\n",
        "    X.append(b_entry)\n",
        "    X.append(nb_entry)\n",
        "\n",
        "  for data in range(sample_size):\n",
        "    y.append(1)\n",
        "    y.append(0)\n",
        "\n",
        "  return X, y\n",
        "\n",
        "X, y = data_from_lists(blurry_stats, nonblurry_stats)"
      ],
      "metadata": {
        "id": "SeLmn7lW3-Br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################################################\n",
        "        # RANK FEATURE IMPORTANCES #\n",
        "##################################################\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "regr = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "regr.fit(X, y)\n",
        "feats = [\"Variance\", \"EM Variance\", \"Sum of Local Variances\", \"EM Sum of Local Variances\", \"Number of Corners\", \n",
        "         \"Average Corner Strength\", \"Entropy\", \"Crofton Perimeter\", \"Standard Deviation\", \"Partial Derivative Variance\"]\n",
        "imps = regr.feature_importances_\n",
        "\n",
        "for i in range(len(feats)):\n",
        "  print(feats[i], \": \", round(imps[i], 2))"
      ],
      "metadata": {
        "id": "9Xt_Rf6F3gO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(b_perims, b_corners, \"ro\", label=\"Blurry\")\n",
        "ax.plot(nb_perims, nb_corners, \"bo\", label=\"Non-Blurry\")\n",
        "plt.style.use('seaborn')\n",
        "plt.title(\"Crofton Perimeter V. Number of Corners\", fontsize=20)\n",
        "plt.xlabel(\"Crofton Perimeter\")\n",
        "plt.ylabel(\"Number of Corners\")\n",
        "fig.set_size_inches(15,15)\n",
        "ax.grid(True)\n",
        "ax.legend()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "4fhhQax-J1e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(b_corneravg, b_corners, \"ro\", label=\"Blurry\")\n",
        "ax.plot(nb_corneravg, nb_corners, \"bo\", label=\"Non-Blurry\")\n",
        "plt.style.use('seaborn')\n",
        "plt.title(\"Average Corner Strength V. Number of Corners\", fontsize=20)\n",
        "plt.xlabel(\"Average Corner Strength\")\n",
        "plt.ylabel(\"Number of Corners\")\n",
        "fig.set_size_inches(15,15)\n",
        "ax.grid(True)\n",
        "ax.legend()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "jK2dDe39Na73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(b_varps, b_corners, \"ro\", label=\"Blurry\")\n",
        "ax.plot(nb_varps, nb_corners, \"bo\", label=\"Non-Blurry\")\n",
        "plt.style.use('seaborn')\n",
        "plt.title(\"Variance of Partial Derivatives V. Number of Corners\", fontsize=20)\n",
        "plt.xlabel(\"Variance of Partial Derivatives\")\n",
        "plt.ylabel(\"Number of Corners\")\n",
        "fig.set_size_inches(15,15)\n",
        "ax.grid(True)\n",
        "ax.legend()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "6ESJ5ewfOJ2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://scikit-learn.org/0.18/auto_examples/svm/plot_iris.html\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm, datasets\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "h = .02  # step size in the mesh\n",
        "\n",
        "# we create an instance of SVM and fit out data. We do not scale our\n",
        "# data since we want to plot the support vectors\n",
        "C = 1.0  # SVM regularization parameter\n",
        "svc = svm.SVC(kernel='linear', C=C).fit(X, y)\n",
        "rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)\n",
        "poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)\n",
        "lin_svc = svm.LinearSVC(C=C).fit(X, y)\n",
        "\n",
        "# create a mesh to plot in\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "\n",
        "# title for the plots\n",
        "titles = ['SVC with linear kernel',\n",
        "          'LinearSVC (linear kernel)',\n",
        "          'SVC with RBF kernel',\n",
        "          'SVC with polynomial (degree 3) kernel']\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "for i, clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):\n",
        "    # Plot the decision boundary. For that, we will assign a color to each\n",
        "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
        "\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "    # Put the result into a color plot\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "\n",
        "    # Plot also the training points\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
        "    plt.xlabel(most_imp[0][0])\n",
        "    plt.ylabel(most_imp[1][0])\n",
        "    plt.xlim(xx.min(), xx.max())\n",
        "    plt.ylim(yy.min(), yy.max())\n",
        "    plt.xticks(())\n",
        "    plt.yticks(())\n",
        "    plt.title(titles[i])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5g5CWe3sjsJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://scikit-learn.org/0.18/auto_examples/svm/plot_iris.html\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm, datasets\n",
        "\n",
        "X, y = data_from_lists([b_corners, b_varps], [nb_corners, nb_varps])\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "h = .02  # step size in the mesh\n",
        "\n",
        "# we create an instance of SVM and fit out data. We do not scale our\n",
        "# data since we want to plot the support vectors\n",
        "C = 1.0  # SVM regularization parameter\n",
        "svc = svm.SVC(kernel='linear', C=C).fit(X, y)\n",
        "rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)\n",
        "poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)\n",
        "lin_svc = svm.LinearSVC(C=C).fit(X, y)\n",
        "\n",
        "# create a mesh to plot in\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "\n",
        "# title for the plots\n",
        "titles = ['SVC with linear kernel',\n",
        "          'LinearSVC (linear kernel)',\n",
        "          'SVC with RBF kernel',\n",
        "          'SVC with polynomial (degree 3) kernel']\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "for i, clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):\n",
        "    # Plot the decision boundary. For that, we will assign a color to each\n",
        "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
        "\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "    # Put the result into a color plot\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "\n",
        "    # Plot also the training points\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
        "    plt.xlabel('Number of Corners')\n",
        "    plt.ylabel('Variance of Partial Derivative')\n",
        "    plt.xlim(xx.min(), xx.max())\n",
        "    plt.ylim(yy.min(), yy.max())\n",
        "    plt.xticks(())\n",
        "    plt.yticks(())\n",
        "    plt.title(titles[i])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IVrppNOjmkhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://scikit-learn.org/0.18/auto_examples/svm/plot_iris.html\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm, datasets\n",
        "\n",
        "X, y = data_from_lists([b_corners, b_corneravg], [nb_corners, nb_corneravg])\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "h = .02  # step size in the mesh\n",
        "\n",
        "# we create an instance of SVM and fit out data. We do not scale our\n",
        "# data since we want to plot the support vectors\n",
        "C = 1.0  # SVM regularization parameter\n",
        "svc = svm.SVC(kernel='linear', C=C).fit(X, y)\n",
        "rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)\n",
        "poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)\n",
        "lin_svc = svm.LinearSVC(C=C).fit(X, y)\n",
        "\n",
        "# create a mesh to plot in\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "\n",
        "# title for the plots\n",
        "titles = ['SVC with linear kernel',\n",
        "          'LinearSVC (linear kernel)',\n",
        "          'SVC with RBF kernel',\n",
        "          'SVC with polynomial (degree 3) kernel']\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "for i, clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):\n",
        "    # Plot the decision boundary. For that, we will assign a color to each\n",
        "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
        "\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "    # Put the result into a color plot\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "\n",
        "    # Plot also the training points\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
        "    plt.xlabel('Number of Corners')\n",
        "    plt.ylabel('Average Corner Strength')\n",
        "    plt.xlim(xx.min(), xx.max())\n",
        "    plt.ylim(yy.min(), yy.max())\n",
        "    plt.xticks(())\n",
        "    plt.yticks(())\n",
        "    plt.title(titles[i])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vhT8d9p5mw8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize Harris Corner Detection\n",
        "Blurry ex: 030FD_042900\n",
        "\n",
        "Nonblurry ex: 030FD_036575"
      ],
      "metadata": {
        "id": "i7yWqQKKoR8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blurry_ex = Image.open(\"Blurry/030FD_042900.jpg\")\n",
        "corner_mat = harris_detection(blurry_ex, 1000)\n",
        "corner_mat = Image.fromarray(corner_mat)\n",
        "corner_mat = corner_mat.convert(\"RGB\")\n",
        "corner_mat"
      ],
      "metadata": {
        "id": "r5saw2GJpK6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blurry_ex = Image.open(\"Non-Blurry/030FD_036575.jpg\")\n",
        "corner_mat = harris_detection(blurry_ex, 1000)\n",
        "corner_mat = Image.fromarray(corner_mat)\n",
        "corner_mat = corner_mat.convert(\"RGB\")\n",
        "corner_mat"
      ],
      "metadata": {
        "id": "MvklNjI30Y4r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}